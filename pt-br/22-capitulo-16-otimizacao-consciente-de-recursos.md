# Cap√≠tulo 16: Otimiza√ß√£o Consciente de Recursos

A Otimiza√ß√£o Consciente de Recursos permite que agentes inteligentes monitorem e gerenciem dinamicamente recursos computacionais, temporais e financeiros durante a opera√ß√£o. Isto difere do planejamento simples, que foca principalmente no sequenciamento de a√ß√µes. A Otimiza√ß√£o Consciente de Recursos requer que agentes tomem decis√µes sobre execu√ß√£o de a√ß√µes para atingir objetivos dentro de or√ßamentos de recursos especificados ou para otimizar efici√™ncia. Isto envolve escolher entre modelos mais precisos mas caros e modelos mais r√°pidos e de menor custo, ou decidir se alocar computa√ß√£o adicional para uma resposta mais refinada versus retornar uma resposta mais r√°pida e menos detalhada.

Por exemplo, considere um agente encarregado de analisar um grande conjunto de dados para um analista financeiro. Se o analista precisa de um relat√≥rio preliminar imediatamente, o agente pode usar um modelo mais r√°pido e acess√≠vel para resumir rapidamente tend√™ncias chave. No entanto, se o analista requer uma previs√£o altamente precisa para uma decis√£o de investimento cr√≠tica e tem um or√ßamento maior e mais tempo, o agente alocaria mais recursos para utilizar um modelo preditivo poderoso, mais lento, mas mais preciso. Uma estrat√©gia chave nesta categoria √© o mecanismo de fallback, que atua como uma salvaguarda quando um modelo preferido est√° indispon√≠vel devido a estar sobrecarregado ou limitado. Para garantir degrada√ß√£o graciosa, o sistema automaticamente muda para um modelo padr√£o ou mais acess√≠vel, mantendo continuidade de servi√ßo ao inv√©s de falhar completamente.

# Aplica√ß√µes Pr√°ticas e Casos de Uso

Casos de uso pr√°ticos incluem:

* **Uso de LLM Otimizado por Custo:** Um agente decidindo se usar um LLM grande e caro para tarefas complexas ou um menor e mais acess√≠vel para consultas mais simples, baseado em uma restri√ß√£o de or√ßamento.  
* **Opera√ß√µes Sens√≠veis √† Lat√™ncia:** Em sistemas em tempo real, um agente escolhe um caminho de racioc√≠nio mais r√°pido mas potencialmente menos abrangente para garantir uma resposta oportuna.  
* **Efici√™ncia Energ√©tica:** Para agentes implantados em dispositivos de borda ou com energia limitada, otimizando seu processamento para conservar vida da bateria.  
* **Fallback para confiabilidade de servi√ßo:** Um agente automaticamente muda para um modelo de backup quando a escolha prim√°ria est√° indispon√≠vel, garantindo continuidade de servi√ßo e degrada√ß√£o graciosa.  
* **Gerenciamento de Uso de Dados:** Um agente optando por recupera√ß√£o de dados resumida ao inv√©s de downloads de conjunto de dados completos para economizar largura de banda ou armazenamento.  
* **Aloca√ß√£o de Tarefas Adaptativa:** Em sistemas multi-agente, agentes se atribuem tarefas baseados em sua carga computacional atual ou tempo dispon√≠vel.

# Exemplo de C√≥digo Hands-On

Um sistema inteligente para responder perguntas de usu√°rios pode avaliar a dificuldade de cada pergunta. Para consultas simples, utiliza um modelo de linguagem custo-efetivo como Gemini Flash. Para consultas complexas, um modelo de linguagem mais poderoso, mas caro (como Gemini Pro) √© considerado. A decis√£o de usar o modelo mais poderoso tamb√©m depende da disponibilidade de recursos, especificamente restri√ß√µes de or√ßamento e tempo. Este sistema seleciona dinamicamente modelos apropriados.

Por exemplo, considere um planejador de viagem constru√≠do com um agente hier√°rquico. O planejamento de alto n√≠vel, que envolve entender a solicita√ß√£o complexa de um usu√°rio, dividindo-a em um itiner√°rio multi-passo e fazendo decis√µes l√≥gicas, seria gerenciado por um LLM sofisticado e mais poderoso como Gemini Pro. Este √© o agente "planejador" que requer uma compreens√£o profunda de contexto e a capacidade de raciocinar.

No entanto, uma vez que o plano √© estabelecido, as tarefas individuais dentro daquele plano, como procurar pre√ßos de voos, verificar disponibilidade de hot√©is ou encontrar avalia√ß√µes de restaurantes, s√£o essencialmente consultas web simples e repetitivas. Estas "chamadas de fun√ß√£o de ferramenta" podem ser executadas por um modelo mais r√°pido e acess√≠vel como Gemini Flash. √â mais f√°cil visualizar por que o modelo acess√≠vel pode ser usado para estas buscas web diretas, enquanto a fase de planejamento intrincado requer a maior intelig√™ncia do modelo mais avan√ßado para garantir um plano de viagem coerente e l√≥gico.

O ADK do Google suporta esta abordagem atrav√©s de sua arquitetura multi-agente, que permite aplica√ß√µes modulares e escal√°veis. Diferentes agentes podem lidar com tarefas especializadas. A flexibilidade de modelo permite o uso direto de v√°rios modelos Gemini, incluindo tanto Gemini Pro quanto Gemini Flash, ou integra√ß√£o de outros modelos atrav√©s de LiteLLM. As capacidades de orquestra√ß√£o do ADK suportam roteamento din√¢mico e dirigido por LLM para comportamento adaptativo. Recursos de avalia√ß√£o integrados permitem avalia√ß√£o sistem√°tica do desempenho do agente, que pode ser usado para refinamento do sistema (veja o Cap√≠tulo sobre Avalia√ß√£o e Monitoramento).

Em seguida, dois agentes com configura√ß√£o id√™ntica mas utilizando modelos e custos diferentes ser√£o definidos.

```python
# Estrutura conceitual tipo Python, n√£o c√≥digo execut√°vel
from google.adk.agents import Agent
# from google.adk.models.lite_llm import LiteLlm  # Se usando modelos n√£o suportados diretamente pelo Agent padr√£o do ADK

# Agente usando o Gemini Pro 2.5 mais caro
gemini_pro_agent = Agent(
    name="GeminiProAgent",
    model="gemini-2.5-pro",  # Placeholder para nome real do modelo se diferente
    description="Um agente altamente capaz para consultas complexas.",
    instruction="Voc√™ √© um assistente especialista para resolu√ß√£o de problemas complexos."
)

# Agente usando o Gemini Flash 2.5 menos caro
gemini_flash_agent = Agent(
    name="GeminiFlashAgent",
    model="gemini-2.5-flash",  # Placeholder para nome real do modelo se diferente
    description="Um agente r√°pido e eficiente para consultas simples.",
    instruction="Voc√™ √© um assistente r√°pido para quest√µes diretas."
)
```

Um Agente Roteador pode direcionar consultas baseadas em m√©tricas simples como comprimento da consulta, onde consultas mais curtas v√£o para modelos menos caros e consultas mais longas para modelos mais capazes. No entanto, um Agente Roteador mais sofisticado pode utilizar modelos LLM ou ML para analisar nuances e complexidade da consulta. Este roteador LLM pode determinar qual modelo de linguagem downstream √© mais adequado. Por exemplo, uma consulta solicitando recall factual √© roteada para um modelo flash, enquanto uma consulta complexa requerendo an√°lise profunda √© roteada para um modelo pro.

T√©cnicas de otimiza√ß√£o podem aprimorar ainda mais a efetividade do roteador LLM. Ajuste de prompt envolve criar prompts para guiar o roteador LLM para melhores decis√µes de roteamento. O fine-tuning do roteador LLM em um conjunto de dados de consultas e suas escolhas de modelo √≥timas melhora sua precis√£o e efici√™ncia. Esta capacidade de roteamento din√¢mico equilibra qualidade de resposta com custo-efetividade.

```python
# Estrutura conceitual tipo Python, n√£o c√≥digo execut√°vel
from google.adk.agents import Agent, BaseAgent
from google.adk.events import Event
from google.adk.agents.invocation_context import InvocationContext
import asyncio

class QueryRouterAgent(BaseAgent):
    name: str = "QueryRouter"
    description: str = "Roteia consultas de usu√°rios para o agente LLM apropriado baseado em complexidade."
    
    async def _run_async_impl(self, context: InvocationContext) -> AsyncGenerator[Event, None]:
        user_query = context.current_message.text  # Assumindo entrada de texto
        query_length = len(user_query.split())  # M√©trica simples: n√∫mero de palavras
        
        if query_length < 20:  # Exemplo de threshold para simplicidade vs. complexidade
            print(f"Roteando para Agente Gemini Flash para consulta curta (comprimento: {query_length})")
            # Em uma configura√ß√£o ADK real, voc√™ 'transferiria_para_agente' ou invocaria diretamente
            # Para demonstra√ß√£o, simularemos uma chamada e retornaremos sua resposta
            response = await gemini_flash_agent.run_async(context.current_message)
            yield Event(author=self.name, content=f"Agente Flash processou: {response}")
        else:
            print(f"Roteando para Agente Gemini Pro para consulta longa (comprimento: {query_length})")
            response = await gemini_pro_agent.run_async(context.current_message)
            yield Event(author=self.name, content=f"Agente Pro processou: {response}")
```

O Agente Cr√≠tico avalia respostas de modelos de linguagem, fornecendo feedback que serve v√°rias fun√ß√µes. Para auto-corre√ß√£o, identifica erros ou inconsist√™ncias, solicitando que o agente respondente refine sua sa√≠da para qualidade aprimorada. Tamb√©m avalia sistematicamente respostas para monitoramento de desempenho, rastreando m√©tricas como precis√£o e relev√¢ncia, que s√£o usadas para otimiza√ß√£o.

Al√©m disso, seu feedback pode sinalizar aprendizado por refor√ßo ou fine-tuning; identifica√ß√£o consistente de respostas inadequadas do modelo Flash, por exemplo, pode refinar a l√≥gica do agente roteador. Embora n√£o gerencie diretamente o or√ßamento, o Agente Cr√≠tico contribui para gerenciamento indireto de or√ßamento identificando escolhas de roteamento sub√≥timas, como direcionar consultas simples para um modelo Pro ou consultas complexas para um modelo Flash, o que leva a resultados pobres. Isto informa ajustes que melhoram aloca√ß√£o de recursos e economia de custos.

O Agente Cr√≠tico pode ser configurado para revisar apenas o texto gerado do agente respondente ou tanto a consulta original quanto o texto gerado, permitindo uma avalia√ß√£o abrangente do alinhamento da resposta com a pergunta inicial.

```text
CRITIC_SYSTEM_PROMPT = """
Voc√™ √© o **Agente Cr√≠tico**, servindo como o bra√ßo de garantia de qualidade de nosso sistema 
assistente de pesquisa colaborativo. 

Sua fun√ß√£o prim√°ria √© **revisar e desafiar meticulosamente** informa√ß√£o do Agente Pesquisador, 
garantindo **precis√£o, completude e apresenta√ß√£o imparcial**.

Seus deveres abrangem:

* **Avaliar descobertas de pesquisa** para corre√ß√£o factual, minuciosidade e poss√≠veis tend√™ncias.

* **Identificar qualquer dado ausente** ou inconsist√™ncias no racioc√≠nio.

* **Levantar quest√µes cr√≠ticas** que poderiam refinar ou expandir o entendimento atual.

* **Oferecer sugest√µes construtivas** para aprimoramento ou explorar diferentes √¢ngulos.

* **Validar que a sa√≠da final √© abrangente** e equilibrada.

Toda cr√≠tica deve ser construtiva. Seu objetivo √© fortificar a pesquisa, n√£o invalid√°-la. 
Estruture seu feedback claramente, chamando aten√ß√£o para pontos espec√≠ficos para revis√£o. 

Seu objetivo geral √© garantir que o produto de pesquisa final atenda aos mais altos padr√µes 
de qualidade poss√≠veis.
"""
```

O Agente Cr√≠tico opera baseado em um prompt de sistema pr√©-definido que delineia seu papel, responsabilidades e abordagem de feedback. Um prompt bem projetado para este agente deve estabelecer claramente sua fun√ß√£o como avaliador. Deve especificar as √°reas para foco cr√≠tico e enfatizar fornecer feedback construtivo ao inv√©s de mera rejei√ß√£o. O prompt tamb√©m deve encorajar a identifica√ß√£o de tanto pontos fortes quanto fracos, e deve guiar o agente sobre como estruturar e apresentar seu feedback.

# C√≥digo Hands-On com OpenAI

Este sistema usa uma estrat√©gia de otimiza√ß√£o consciente de recursos para lidar com consultas de usu√°rios eficientemente. Primeiro classifica cada consulta em uma de tr√™s categorias para determinar o caminho de processamento mais apropriado e custo-efetivo. Esta abordagem evita desperdi√ßar recursos computacionais em solicita√ß√µes simples enquanto garante que consultas complexas recebam a aten√ß√£o necess√°ria. As tr√™s categorias s√£o:

* simple: Para quest√µes diretas que podem ser respondidas diretamente sem racioc√≠nio complexo ou dados externos.  
* reasoning: Para consultas que requerem dedu√ß√£o l√≥gica ou processos de pensamento multi-passo, que s√£o roteadas para modelos mais poderosos.  
* internet_search: Para quest√µes precisando de informa√ß√£o atual, que automaticamente aciona uma Busca Google para fornecer uma resposta atualizada.

O c√≥digo est√° sob licen√ßa MIT e dispon√≠vel no Github: ([https://github.com/mahtabsyed/21-Agentic-Patterns/blob/main/16_Resource_Aware_Opt_LLM_Reflection_v2.ipynb](https://github.com/mahtabsyed/21-Agentic-Patterns/blob/main/16_Resource_Aware_Opt_LLM_Reflection_v2.ipynb))

```python
# Licen√ßa MIT
# Copyright (c) 2025 Mahtab Syed
# https://www.linkedin.com/in/mahtabsyed/

import os
import requests
import json
from dotenv import load_dotenv
from openai import OpenAI

# Carregar vari√°veis de ambiente
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_CUSTOM_SEARCH_API_KEY = os.getenv("GOOGLE_CUSTOM_SEARCH_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID")

if not OPENAI_API_KEY or not GOOGLE_CUSTOM_SEARCH_API_KEY or not GOOGLE_CSE_ID:
    raise ValueError(
        "Por favor, defina OPENAI_API_KEY, GOOGLE_CUSTOM_SEARCH_API_KEY e GOOGLE_CSE_ID em seu arquivo .env."
    )

client = OpenAI(api_key=OPENAI_API_KEY) # --- Passo 1: Classificar o Prompt ---
def classify_prompt(prompt: str) -> dict:
    system_message = {
        "role": "system", 
        "content": (
            "Voc√™ √© um classificador que analisa prompts de usu√°rios e retorna uma de tr√™s categorias APENAS:\n\n"
            "- simple\n"
            "- reasoning\n"
            "- internet_search\n\n"
            "Regras:\n"
            "- Use 'simple' para quest√µes factuais diretas que n√£o precisam de racioc√≠nio ou eventos atuais.\n"
            "- Use 'reasoning' para quest√µes de l√≥gica, matem√°tica ou infer√™ncia multi-passo.\n"
            "- Use 'internet_search' se o prompt se refere a eventos atuais, dados recentes ou coisas n√£o em seus dados de treinamento.\n\n"
            "Responda APENAS com JSON como:\n"
            '{"classification": "simple"}'
        ),
    }
    
    user_message = {
        "role": "user", 
        "content": prompt
    }
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[system_message, user_message],
        temperature=1
    )
    
    reply = response.choices[0].message.content
    return json.loads(reply)

# --- Passo 2: Busca Google ---
def google_search(query: str, num_results=1) -> list:
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": GOOGLE_CUSTOM_SEARCH_API_KEY,
        "cx": GOOGLE_CSE_ID,
        "q": query,
        "num": num_results,
    }
    
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        results = response.json()
        
        if "items" in results and results["items"]:
            return [
                {
                    "title": item.get("title"),
                    "snippet": item.get("snippet"),
                    "link": item.get("link"),
                }
                for item in results["items"]
            ]
        else:
            return []
    except requests.exceptions.RequestException as e:
        return {"error": str(e)}

# --- Passo 3: Gerar Resposta ---
def generate_response(prompt: str, classification: str, search_results=None) -> str:
    if classification == "simple":
        model = "gpt-4o-mini"
        full_prompt = prompt
    elif classification == "reasoning":
        model = "o4-mini"
        full_prompt = prompt
    elif classification == "internet_search":
        model = "gpt-4o"
        
        # Converter cada resultado de busca dict para uma string leg√≠vel
        if search_results:
            search_context = "\n".join(
                [
                    f"T√≠tulo: {item.get('title')}\n"
                    f"Snippet: {item.get('snippet')}\n"
                    f"Link: {item.get('link')}\n"
                    for item in search_results
                ]
            )
        else:
            search_context = "Nenhum resultado de busca encontrado."
        
        full_prompt = f"""Use os seguintes resultados da web para responder √† consulta do usu√°rio:
{search_context}

Consulta: {prompt}
"""
    
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "user", "content": full_prompt}
        ],
        temperature=1,
    )
    
    return response.choices[0].message.content, model

# --- Passo 4: Roteador Combinado ---
def handle_prompt(prompt: str) -> dict:
    classification_result = classify_prompt(prompt)
    # Remover ou comentar a pr√≥xima linha para evitar impress√£o duplicada
    # print("\nüîç Resultado da Classifica√ß√£o:", classification_result)
    
    classification = classification_result["classification"]
    search_results = None
    
    if classification == "internet_search":
        search_results = google_search(prompt)
        # print("\nüîç Resultados da Busca:", search_results)
    
    answer, model = generate_response(prompt, classification, search_results)
    
    return {
        "classification": classification,
        "response": answer,
        "model": model
    }

# Teste do sistema
test_prompt = "Qual √© a capital da Austr√°lia?"
# test_prompt = "Explique o impacto da computa√ß√£o qu√¢ntica na criptografia."
# test_prompt = "Quando come√ßa o Australian Open 2026, me d√™ a data completa?"

result = handle_prompt(test_prompt)
print("üîç Classifica√ß√£o:", result["classification"])
print("üß† Modelo Usado:", result["model"])
print("üß† Resposta:\n", result["response"])
```

Este c√≥digo Python implementa um sistema de roteamento de prompt para responder perguntas de usu√°rios. Come√ßa carregando chaves de API necess√°rias de um arquivo .env para OpenAI e Google Custom Search. A funcionalidade central est√° em classificar o prompt do usu√°rio em tr√™s categorias: simple, reasoning ou internet search. Uma fun√ß√£o dedicada utiliza um modelo OpenAI para este passo de classifica√ß√£o. Se o prompt requer informa√ß√£o atual, uma busca Google √© realizada usando a API Google Custom Search. Outra fun√ß√£o ent√£o gera a resposta final, selecionando um modelo OpenAI apropriado baseado na classifica√ß√£o. Para consultas de internet search, os resultados da busca s√£o fornecidos como contexto ao modelo. A fun√ß√£o principal handle_prompt orquestra este fluxo de trabalho, chamando as fun√ß√µes de classifica√ß√£o e busca (se necess√°rio) antes de gerar a resposta. Retorna a classifica√ß√£o, o modelo usado e a resposta gerada. Este sistema direciona eficientemente diferentes tipos de consultas para m√©todos otimizados para uma melhor resposta.

# Exemplo de C√≥digo Hands-On (OpenRouter)

OpenRouter oferece uma interface unificada para centenas de modelos de IA via um √∫nico endpoint de API. Fornece failover automatizado e otimiza√ß√£o de custos, com integra√ß√£o f√°cil atrav√©s de seu SDK ou framework preferido.

```python
import requests
import json

response = requests.post(
    url="https://openrouter.ai/api/v1/chat/completions",
    headers={
        "Authorization": "Bearer <OPENROUTER_API_KEY>",
        "HTTP-Referer": "<YOUR_SITE_URL>",  # Opcional. URL do site para rankings no openrouter.ai.
        "X-Title": "<YOUR_SITE_NAME>",      # Opcional. T√≠tulo do site para rankings no openrouter.ai.
    },
    data=json.dumps({
        "model": "openai/gpt-4o",  # Opcional
        "messages": [
            {
                "role": "user",
                "content": "Qual √© o significado da vida?"
            }
        ]
    })
)
```

Este trecho de c√≥digo usa a biblioteca requests para interagir com a API OpenRouter. Envia uma requisi√ß√£o POST para o endpoint de conclus√£o de chat com uma mensagem de usu√°rio. A requisi√ß√£o inclui headers de autoriza√ß√£o com uma chave de API e informa√ß√£o opcional do site. O objetivo √© obter uma resposta de um modelo de linguagem especificado, neste caso, "openai/gpt-4o".

Openrouter oferece duas metodologias distintas para roteamento e determina√ß√£o do modelo computacional usado para processar uma requisi√ß√£o dada.

* **Sele√ß√£o de Modelo Automatizada:** Esta fun√ß√£o roteia uma requisi√ß√£o para um modelo otimizado escolhido de um conjunto curado de modelos dispon√≠veis. A sele√ß√£o √© baseada no conte√∫do espec√≠fico do prompt do usu√°rio. O identificador do modelo que finalmente processa a requisi√ß√£o √© retornado nos metadados da resposta.

```json
{
"model": "openrouter/auto", ... // Outros params }
```

* **Fallback de Modelo Sequencial:** Este mecanismo fornece redund√¢ncia operacional permitindo que usu√°rios especifiquem uma lista hier√°rquica de modelos. O sistema primeiro tentar√° processar a requisi√ß√£o com o modelo prim√°rio designado na sequ√™ncia. Se este modelo prim√°rio falhar em responder devido a qualquer n√∫mero de condi√ß√µes de erro‚Äîcomo indisponibilidade de servi√ßo, limita√ß√£o de taxa ou filtragem de conte√∫do‚Äîo sistema automaticamente re-rotear√° a requisi√ß√£o para o pr√≥ximo modelo especificado na sequ√™ncia. Este processo continua at√© que um modelo na lista execute com sucesso a requisi√ß√£o ou a lista seja esgotada. O custo final da opera√ß√£o e o identificador do modelo retornado na resposta corresponder√£o ao modelo que completou com sucesso a computa√ß√£o.

```json
{
"models": ["anthropic/claude-3.5-sonnet", "gryphe/mythomax-l2-13b"], ... // Outros params }
```

OpenRouter oferece um leaderboard detalhado ([https://openrouter.ai/rankings](https://openrouter.ai/rankings)) que classifica modelos de IA dispon√≠veis baseados em sua produ√ß√£o cumulativa de tokens. Tamb√©m oferece modelos mais recentes de diferentes provedores (ChatGPT, Gemini, Claude) (veja Fig. 1)

![][image1]  
Fig. 1: Site OpenRouter ([https://openrouter.ai/](https://openrouter.ai/))

# Al√©m da Mudan√ßa Din√¢mica de Modelo: Um Espectro de Otimiza√ß√µes de Recursos de Agente

Otimiza√ß√£o consciente de recursos √© primordial no desenvolvimento de sistemas de agentes inteligentes que operam eficientemente e efetivamente dentro de restri√ß√µes do mundo real. Vamos ver v√°rias t√©cnicas adicionais:

**Mudan√ßa Din√¢mica de Modelo** √© uma t√©cnica cr√≠tica envolvendo a sele√ß√£o estrat√©gica de grandes modelos de linguagem baseada nas intricacidades da tarefa em quest√£o e os recursos computacionais dispon√≠veis. Quando confrontado com consultas simples, um LLM leve e custo-efetivo pode ser implantado, enquanto problemas complexos e multifacetados necessitam a utiliza√ß√£o de modelos mais sofisticados e intensivos em recursos.

**Uso e Sele√ß√£o de Ferramentas Adaptativa** garante que agentes possam escolher inteligentemente de uma su√≠te de ferramentas, selecionando a mais apropriada e eficiente para cada sub-tarefa espec√≠fica, com considera√ß√£o cuidadosa dada a fatores como custos de uso de API, lat√™ncia e tempo de execu√ß√£o. Esta sele√ß√£o din√¢mica de ferramentas aprimora a efici√™ncia geral do sistema otimizando o uso de APIs e servi√ßos externos.

**Poda e Resumo Contextual** desempenha um papel vital no gerenciamento da quantidade de informa√ß√£o processada por agentes, minimizando estrategicamente a contagem de tokens de prompt e reduzindo custos de infer√™ncia resumindo inteligentemente e retendo seletivamente apenas a informa√ß√£o mais relevante do hist√≥rico de intera√ß√£o, prevenindo overhead computacional desnecess√°rio.

**Predi√ß√£o Proativa de Recursos** envolve agentes antecipando necessidades futuras de recursos baseadas em padr√µes hist√≥ricos e demandas atuais, permitindo aloca√ß√£o preventiva e otimiza√ß√£o de recursos antes que restri√ß√µes se tornem limitantes.

**Explora√ß√£o Sens√≠vel a Custo em Sistemas Multi-Agente** refere-se a agentes tomando decis√µes sobre explora√ß√£o de novos caminhos ou estrat√©gias baseadas em custos computacionais esperados versus benef√≠cios potenciais, especialmente em ambientes onde m√∫ltiplos agentes competem por recursos limitados.

**Implanta√ß√£o Eficiente em Energia** √© crucial para agentes operando em dispositivos com recursos limitados, onde otimiza√ß√µes focam em minimizar consumo de energia atrav√©s de t√©cnicas como quantiza√ß√£o de modelo, pruning de rede neural e execu√ß√£o de infer√™ncia otimizada.

**Consci√™ncia de Paraleliza√ß√£o e Computa√ß√£o Distribu√≠da** permite que agentes identifiquem oportunidades para dividir tarefas em componentes paralelos ou distribu√≠dos, maximizando utiliza√ß√£o de recursos dispon√≠veis e reduzindo tempo total de execu√ß√£o.

**Pol√≠ticas de Aloca√ß√£o de Recursos Aprendidas** envolvem agentes desenvolvendo estrat√©gias de otimiza√ß√£o de recursos atrav√©s de aprendizado por refor√ßo ou an√°lise de padr√µes hist√≥ricos, melhorando continuamente suas decis√µes de aloca√ß√£o de recursos ao longo do tempo.

**Degrada√ß√£o Graciosa e Mecanismos de Fallback** garantem que agentes possam manter funcionalidade b√°sica mesmo quando recursos se tornam limitados, automaticamente mudando para modos de opera√ß√£o menos intensivos em recursos quando necess√°rio.

**Prioriza√ß√£o de Tarefas Cr√≠ticas** permite que agentes identifiquem e priorizem tarefas essenciais quando recursos s√£o limitados, garantindo que opera√ß√µes cr√≠ticas recebam recursos adequados mesmo em condi√ß√µes de restri√ß√£o.


**Diagrama de Otimiza√ß√£o**

# Principais Conclus√µes

* Otimiza√ß√£o Consciente de Recursos √© Essencial: Agentes inteligentes podem gerenciar recursos computacionais, temporais e financeiros dinamicamente. Decis√µes sobre uso de modelo e caminhos de execu√ß√£o s√£o feitas baseadas em restri√ß√µes e objetivos em tempo real.  
* Arquitetura Multi-Agente para Escalabilidade: O ADK do Google fornece um framework multi-agente, permitindo design modular. Diferentes agentes (respondente, roteador, cr√≠tico) lidam com tarefas espec√≠ficas.  
* Roteamento Din√¢mico Dirigido por LLM: Um Agente Roteador direciona consultas para modelos de linguagem (Gemini Flash para simples, Gemini Pro para complexas) baseado em complexidade de consulta e or√ßamento. Isto otimiza custo e desempenho.  
* Funcionalidade do Agente Cr√≠tico: Um Agente Cr√≠tico dedicado fornece feedback para auto-corre√ß√£o, monitoramento de desempenho e refinamento de l√≥gica de roteamento, aprimorando efetividade do sistema.  
* Otimiza√ß√£o Atrav√©s de Feedback e Flexibilidade: Capacidades de avalia√ß√£o para cr√≠tica e flexibilidade de integra√ß√£o de modelo contribuem para comportamento adaptativo e auto-aprimorador do sistema.  
* Otimiza√ß√µes Adicionais Conscientes de Recursos: Outros m√©todos incluem Uso e Sele√ß√£o de Ferramentas Adaptativa, Poda e Resumo Contextual, Predi√ß√£o Proativa de Recursos, Explora√ß√£o Sens√≠vel a Custo em Sistemas Multi-Agente, Implanta√ß√£o Eficiente em Energia, Consci√™ncia de Paraleliza√ß√£o e Computa√ß√£o Distribu√≠da, Pol√≠ticas de Aloca√ß√£o de Recursos Aprendidas, Degrada√ß√£o Graciosa e Mecanismos de Fallback, e Prioriza√ß√£o de Tarefas Cr√≠ticas.

# Conclus√µes

Otimiza√ß√£o consciente de recursos √© essencial para o desenvolvimento de agentes inteligentes, permitindo opera√ß√£o eficiente dentro de restri√ß√µes do mundo real. Ao gerenciar recursos computacionais, temporais e financeiros, agentes podem atingir desempenho √≥timo e custo-efetividade. T√©cnicas como mudan√ßa din√¢mica de modelo, uso adaptativo de ferramentas e poda contextual s√£o cruciais para atingir estas efici√™ncias. Estrat√©gias avan√ßadas, incluindo pol√≠ticas de aloca√ß√£o de recursos aprendidas e degrada√ß√£o graciosa, aprimoram a adaptabilidade e resili√™ncia de um agente sob condi√ß√µes vari√°veis. Integrar estes princ√≠pios de otimiza√ß√£o no design de agentes √© fundamental para construir sistemas de IA escal√°veis, robustos e sustent√°veis.

# Refer√™ncias

1. Google's Agent Development Kit (ADK): [https://google.github.io/adk-docs/](https://google.github.io/adk-docs/)   
2. Gemini Flash 2.5 & Gemini 2.5 Pro:  [https://aistudio.google.com/](https://aistudio.google.com/)   
3. OpenRouter: [https://openrouter.ai/docs/quickstart](https://openrouter.ai/docs/quickstart)

[image1]: ../assets/21-chapter-16-image-1-line-147.png
