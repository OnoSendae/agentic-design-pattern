# Apêndice B - Interações Agentivas de IA: Do GUI ao Ambiente do Mundo Real

Agentes de IA estão cada vez mais executando tarefas complexas interagindo com interfaces digitais e o mundo físico. Sua capacidade de perceber, processar e agir dentro desses ambientes variados está transformando fundamentalmente a automação, interação humano-computador e sistemas inteligentes. Este apêndice explora como agentes interagem com computadores e seus ambientes, destacando avanços e projetos.

# Interação: Agentes com Computadores

A evolução da IA de parceiros conversacionais para agentes ativos e orientados a tarefas está sendo impulsionada por Interfaces Agente-Computador (ACIs). Essas interfaces permitem que a IA interaja diretamente com a Interface Gráfica do Usuário (GUI) de um computador, habilitando-a a perceber e manipular elementos visuais como ícones e botões, assim como um humano faria. Este novo método vai além dos scripts rígidos e dependentes de desenvolvedor da automação tradicional que dependia de APIs e chamadas de sistema. Ao usar a "porta da frente" visual do software, a IA pode agora automatizar tarefas digitais complexas de uma forma mais flexível e poderosa, um processo que envolve várias etapas-chave:

* **Percepção Visual**: O agente primeiro captura uma representação visual da tela, essencialmente tirando uma captura de tela.  
* **Reconhecimento de Elementos GUI**: Ele então analisa esta imagem para distinguir entre vários elementos GUI. Deve aprender a "ver" a tela não como uma mera coleção de pixels, mas como um layout estruturado com componentes interativos, discernindo um botão clicável "Enviar" de uma imagem de banner estática ou um campo de texto editável de um simples rótulo.  
* **Interpretação Contextual**: O módulo ACI, atuando como uma ponte entre os dados visuais e a inteligência central do agente (frequentemente um Large Language Model ou LLM), interpreta esses elementos dentro do contexto da tarefa. Entende que um ícone de lupa tipicamente significa "buscar" ou que uma série de botões de rádio representa uma escolha. Este módulo é crucial para aprimorar o raciocínio do LLM, permitindo-lhe formar um plano baseado em evidência visual.  
* **Ação e Resposta Dinâmica**: O agente então controla programaticamente o mouse e teclado para executar seu plano—clicando, digitando, rolando e arrastando. Criticamente, deve constantemente monitorar a tela para feedback visual, respondendo dinamicamente a mudanças, telas de carregamento, notificações pop-up ou erros para navegar com sucesso workflows multi-etapas.

Esta tecnologia não é mais teórica. Vários laboratórios de IA líderes desenvolveram agentes funcionais que demonstram o poder da interação GUI:

**ChatGPT Operator (OpenAI)**: Concebido como um parceiro digital, o ChatGPT Operator é projetado para automatizar tarefas através de uma ampla gama de aplicações diretamente da área de trabalho. Ele entende elementos na tela, habilitando-o a executar ações como transferir dados de uma planilha para uma plataforma de gerenciamento de relacionamento com cliente (CRM), reservar um itinerário de viagem complexo através de sites de companhias aéreas e hotéis, ou preencher formulários online detalhados sem precisar de acesso especializado à API para cada serviço. Isso o torna uma ferramenta universalmente adaptável visando impulsionar tanto a produtividade pessoal quanto empresarial assumindo tarefas digitais repetitivas.

**Google Project Mariner**: Como um protótipo de pesquisa, o Project Mariner opera como um agente dentro do navegador Chrome (veja Fig. 1). Seu propósito é entender a intenção do usuário e executar autonomamente tarefas baseadas na web em seu nome. Por exemplo, um usuário poderia pedir-lhe para encontrar três apartamentos para alugar dentro de um orçamento e bairro específicos; Mariner então navegaria para sites imobiliários, aplicaria os filtros, navegaria pelos anúncios e extrairia a informação relevante em um documento. Este projeto representa a exploração do Google em criar uma experiência web verdadeiramente útil e "agentiva" onde o navegador trabalha ativamente para o usuário.

![][image1]

Fig.1: Interação entre um Agente e o Navegador Web

**Computer Use da Anthropic**: Este recurso capacita o modelo de IA da Anthropic, Claude, a se tornar um usuário direto do ambiente de área de trabalho de um computador. Ao capturar capturas de tela para perceber a tela e controlar programaticamente o mouse e teclado, Claude pode orquestrar workflows que abrangem múltiplas aplicações não conectadas. Um usuário poderia pedir-lhe para analisar dados em um relatório PDF, abrir uma aplicação de planilha para executar cálculos nesses dados, gerar um gráfico, e então colar esse gráfico em um rascunho de email—uma sequência de tarefas que anteriormente requeriam entrada humana constante.

**Browser Use**: Esta é uma biblioteca open-source que fornece uma API de alto nível para automação programática de navegador. Ela habilita agentes de IA a interfacear com páginas web concedendo-lhes acesso e controle sobre o Document Object Model (DOM). A API abstrai os comandos intrincados e de baixo nível dos protocolos de controle de navegador em um conjunto mais simplificado e intuitivo de funções. Isso permite que um agente execute sequências complexas de ações, incluindo extração de dados de elementos aninhados, submissões de formulário e navegação automatizada através de múltiplas páginas. Como resultado, a biblioteca facilita a transformação de dados web não estruturados em um formato estruturado que um agente de IA pode processar e utilizar sistematicamente para análise ou tomada de decisão.

# Interação: Agentes com o Ambiente

Além dos confins de uma tela de computador, agentes de IA estão cada vez mais sendo projetados para interagir com ambientes complexos e dinâmicos, frequentemente espelhando o mundo real. Isso requer capacidades sofisticadas de percepção, raciocínio e atuação.

O **Project Astra** do Google é um exemplo principal de uma iniciativa empurrando os limites da interação de agentes com o ambiente. Astra visa criar um agente de IA universal que é útil na vida cotidiana, aproveitando entradas multimodais (visão, som, voz) e saídas para entender e interagir com o mundo contextualmente. Este projeto foca em entendimento rápido, raciocínio e resposta, permitindo que o agente "veja" e "ouça" seus arredores através de câmeras e microfones e se envolva em conversa natural enquanto fornece assistência em tempo real. A visão do Astra é um agente que pode assistir perfeitamente usuários com tarefas que variam de encontrar itens perdidos a debugar código, entendendo o ambiente que observa. Isso vai além de comandos de voz simples para um entendimento verdadeiramente incorporado do contexto físico imediato do usuário.

O **Gemini Live** do Google transforma interações padrão de IA em uma conversa fluida e dinâmica. Usuários podem falar com a IA e receber respostas em uma voz de som natural com atraso mínimo, e podem até interromper ou mudar tópicos no meio da frase, fazendo a IA se adaptar imediatamente. A interface se expande além da voz, permitindo que usuários incorporem informação visual usando a câmera de seu telefone, compartilhando sua tela, ou fazendo upload de arquivos para uma discussão mais consciente do contexto. Versões mais avançadas podem até perceber o tom de voz do usuário e filtrar inteligentemente ruído de fundo irrelevante para entender melhor a conversa. Essas capacidades se combinam para criar interações ricas, como receber instruções ao vivo sobre uma tarefa simplesmente apontando uma câmera para ela.

O **modelo GPT-4o da OpenAI** é uma alternativa projetada para interação "omni", significando que pode raciocinar através de voz, visão e texto. Ele processa essas entradas com baixa latência que espelha tempos de resposta humanos, o que permite conversas em tempo real. Por exemplo, usuários podem mostrar à IA um feed de vídeo ao vivo para fazer perguntas sobre o que está acontecendo, ou usá-la para tradução de idioma. A OpenAI fornece aos desenvolvedores uma "Realtime API" para construir aplicações requerendo interações de baixa latência, fala-para-fala.

O **ChatGPT Agent da OpenAI** representa um avanço arquitetural significativo sobre seus predecessores, apresentando um framework integrado de novas capacidades. Seu design incorpora várias modalidades funcionais-chave: a capacidade para navegação autônoma da internet ao vivo para extração de dados em tempo real, a habilidade de gerar e executar dinamicamente código computacional para tarefas como análise de dados, e a funcionalidade para interfacear diretamente com aplicações de software de terceiros. A síntese dessas funções permite que o agente orquestre e complete workflows complexos e sequenciais de uma diretiva singular do usuário. Ele pode portanto gerenciar autonomamente processos inteiros, como executar análise de mercado e gerar uma apresentação correspondente, ou planejar arranjos logísticos e executar as transações necessárias. Em paralelo com o lançamento, a OpenAI abordou proativamente as considerações de segurança emergentes inerentes a tal sistema. Um "System Card" acompanhante delineia os perigos operacionais potenciais associados a uma IA capaz de executar ações online, reconhecendo os novos vetores para uso indevido. Para mitigar esses riscos, a arquitetura do agente inclui salvaguardas projetadas, como requerer autorização explícita do usuário para certas classes de ações e implantar mecanismos robustos de filtragem de conteúdo. A empresa está agora engajando sua base de usuários inicial para refinar ainda mais esses protocolos de segurança através de um processo iterativo dirigido por feedback.

**Seeing AI**, uma aplicação móvel complementar da Microsoft, capacita indivíduos que são cegos ou têm baixa visão oferecendo narração em tempo real de seus arredores. O app aproveita inteligência artificial através da câmera do dispositivo para identificar e descrever vários elementos, incluindo objetos, texto e até pessoas. Suas funcionalidades centrais abrangem ler documentos, reconhecer moeda, identificar produtos através de códigos de barras e descrever cenas e cores. Ao fornecer acesso aprimorado à informação visual, Seeing AI em última análise promove maior independência para usuários com deficiência visual.

**Série Claude 4 da Anthropic** O Claude 4 da Anthropic é outra alternativa com capacidades para raciocínio e análise avançados. Embora historicamente focado em texto, Claude 4 inclui capacidades robustas de visão, permitindo-lhe processar informação de imagens, gráficos e documentos. O modelo é adequado para lidar com tarefas complexas e multi-etapas e fornecer análise detalhada. Embora o aspecto conversacional em tempo real não seja seu foco primário comparado a outros modelos, sua inteligência subjacente é projetada para construir agentes de IA altamente capazes.

# Vibe Coding: Desenvolvimento Intuitivo com IA

Além da interação direta com GUIs e o mundo físico, um novo paradigma está emergindo em como desenvolvedores constroem software com IA: "vibe coding." Esta abordagem se afasta de instruções precisas e passo a passo e em vez disso depende de uma interação mais intuitiva, conversacional e iterativa entre o desenvolvedor e um assistente de codificação de IA. O desenvolvedor fornece um objetivo de alto nível, um "vibe" desejado, ou uma direção geral, e a IA gera código para corresponder.

Este processo é caracterizado por:

- **Prompts Conversacionais**: Em vez de escrever especificações detalhadas, um desenvolvedor pode dizer, "Crie uma página de destino simples e moderna para um novo app," ou, "Refatore esta função para ser mais Pythonica e legível." A IA interpreta o "vibe" de "moderna" ou "Pythonica" e gera o código correspondente.

- **Refinamento Iterativo**: A saída inicial da IA é frequentemente um ponto de partida. O desenvolvedor então fornece feedback em linguagem natural, como, "É um bom começo, mas você pode tornar os botões azuis?" ou, "Adicione algum tratamento de erro a isso." Este vai-e-vem continua até que o código atenda às expectativas do desenvolvedor.
- **Parceria Criativa**: No vibe coding, a IA atua como um parceiro criativo, sugerindo ideias e soluções que o desenvolvedor pode não ter considerado. Isso pode acelerar o processo de desenvolvimento e levar a resultados mais inovadores.
- **Foco no "O quê" não "Como"**: O desenvolvedor se foca no resultado desejado (o "o quê") e deixa os detalhes de implementação (o "como") para a IA. Isso permite prototipagem rápida e exploração de diferentes abordagens sem se perder em código boilerplate.
- **Bancos de Memória Opcionais**: Para manter contexto através de interações mais longas, desenvolvedores podem usar "bancos de memória" para armazenar informação-chave, preferências ou restrições. Por exemplo, um desenvolvedor pode salvar um estilo de codificação específico ou um conjunto de requisitos de projeto na memória da IA, garantindo que gerações futuras de código permaneçam consistentes com o "vibe" estabelecido sem precisar repetir as instruções.

O vibe coding está se tornando cada vez mais popular com o surgimento de modelos de IA poderosos como GPT-4, Claude e Gemini, que são integrados em ambientes de desenvolvimento. Essas ferramentas não estão apenas auto-completando código; elas estão participando ativamente no processo criativo de desenvolvimento de software, tornando-o mais acessível e eficiente. Esta nova forma de trabalhar está mudando a natureza da engenharia de software, enfatizando criatividade e pensamento de alto nível sobre memorização mecânica de sintaxe e APIs.

# Principais conclusões

* Agentes de IA estão evoluindo de automação simples para controlar visualmente software através de interfaces gráficas do usuário, muito como um humano faria.
* A próxima fronteira é interação com o mundo real, com projetos como o Astra do Google usando câmeras e microfones para ver, ouvir e entender seus arredores físicos.
* Empresas de tecnologia líderes estão convergindo essas capacidades digitais e físicas para criar assistentes de IA universais que operam perfeitamente em ambos os domínios.
* Esta mudança está criando uma nova classe de companheiros de IA proativos e conscientes do contexto capazes de assistir com uma vasta gama de tarefas na vida diária dos usuários.

# Conclusão

Agentes estão passando por uma transformação significativa, movendo-se de automação básica para interação sofisticada com ambientes digitais e físicos. Ao aproveitar percepção visual para operar Interfaces Gráficas do Usuário, esses agentes podem agora manipular software assim como um humano faria, contornando a necessidade de APIs tradicionais. Laboratórios de tecnologia principais estão pioneirando este espaço com agentes capazes de automatizar workflows complexos e de múltiplas aplicações diretamente na área de trabalho de um usuário. Simultaneamente, a próxima fronteira está se expandindo para o mundo físico, com iniciativas como o Project Astra do Google usando câmeras e microfones para se engajar contextualmente com seus arredores. Esses sistemas avançados são projetados para entendimento multimodal e em tempo real que espelha interação humana.

A visão final é uma convergência dessas capacidades digitais e físicas, criando assistentes de IA universais que operam perfeitamente através de todos os ambientes de um usuário. Esta evolução também está remodelando a criação de software em si através do "vibe coding," uma parceria mais intuitiva e conversacional entre desenvolvedores e IA. Este novo método prioriza objetivos de alto nível e intenção criativa, permitindo que desenvolvedores se foquem no resultado desejado ao invés de detalhes de implementação. Esta mudança acelera desenvolvimento e promove inovação tratando IA como um parceiro criativo. Em última análise, esses avanços estão pavimentando o caminho para uma nova era de companheiros de IA proativos e conscientes do contexto capazes de assistir com uma vasta gama de tarefas em nossas vidas diárias.

# Referências

1. Open AI Operator, [https://openai.com/index/introducing-operator/](https://openai.com/index/introducing-operator/)   
2. Open AI ChatGPT Agent: [https://openai.com/index/introducing-chatgpt-agent/](https://openai.com/index/introducing-chatgpt-agent/)   
3. Browser Use: [https://docs.browser-use.com/introduction](https://docs.browser-use.com/introduction)   
4. Project Mariner, [https://deepmind.google/models/project-mariner/](https://deepmind.google/models/project-mariner/)   
5. Anthropic Computer use: [https://docs.anthropic.com/en/docs/build-with-claude/computer-use](https://docs.anthropic.com/en/docs/build-with-claude/computer-use)  
6. Project Astra, [https://deepmind.google/models/project-astra/](https://deepmind.google/models/project-astra/)   
7. Gemini Live, [https://gemini.google/overview/gemini-live/?hl=en](https://gemini.google/overview/gemini-live/?hl=en)   
8. OpenAI's GPT-4,  [https://openai.com/index/gpt-4-research/](https://openai.com/index/gpt-4-research/)   
9. Claude 4, [https://www.anthropic.com/news/claude-4](https://www.anthropic.com/news/claude-4)
